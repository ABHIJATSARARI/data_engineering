{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# PLEASE NOTE: Please run this notebook OUTSIDE a Spark notebook as it should run in a plain Default Python 3.6 Free Environment\n\nThis is the last assignment for the Coursera course \"Advanced Machine Learning and Signal Processing\"\n\nJust execute all cells one after the other and you are done - just note that in the last one you should update your email address (the one you've used for coursera) and obtain a submission token, you get this from the programming assignment directly on coursera.\n\nPlease fill in the sections labelled with \"###YOUR_CODE_GOES_HERE###\"\n\nThe purpose of this assignment is to learn how feature engineering boosts model performance. You will apply Discrete Fourier Transformation on the accelerometer sensor time series and therefore transforming the dataset from the time to the frequency domain. \n\nAfter that, you\u2019ll use a classification algorithm of your choice to create a model and submit the new predictions to the grader. Done.\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n    \n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n  Using cached pyspark-2.4.5-py2.py3-none-any.whl\nCollecting py4j==0.10.7\n  Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB)\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.2\n    Uninstalling py4j-0.10.9.2:\n      Successfully uninstalled py4j-0.10.9.2\n  Attempting uninstall: pyspark\n    Found existing installation: pyspark 3.2.0\n    Uninstalling pyspark-3.2.0:\n      Successfully uninstalled pyspark-3.2.0\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pyspark in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (2.4.5)\nCollecting pyspark\n  Using cached pyspark-3.2.0-py2.py3-none-any.whl\nCollecting py4j==0.10.9.2\n  Using cached py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.7\n    Uninstalling py4j-0.10.7:\n      Successfully uninstalled py4j-0.10.7\n  Attempting uninstall: pyspark\n    Found existing installation: pyspark 2.4.5\n    Uninstalling pyspark-2.4.5:\n      Successfully uninstalled pyspark-2.4.5\nSuccessfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
                }
            ],
            "source": "!pip install --upgrade pyspark"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting https://github.com/IBM/coursera/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true\n  Using cached https://github.com/IBM/coursera/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true (9.9 MB)\nRequirement already satisfied: numpy>=1.8.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from systemml==1.3.0) (1.19.2)\nRequirement already satisfied: scipy>=0.15.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from systemml==1.3.0) (1.4.1)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from systemml==1.3.0) (1.2.4)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from systemml==1.3.0) (0.23.2)\nRequirement already satisfied: Pillow>=2.0.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from systemml==1.3.0) (8.4.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pandas->systemml==1.3.0) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pandas->systemml==1.3.0) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->systemml==1.3.0) (1.15.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from scikit-learn->systemml==1.3.0) (0.17.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from scikit-learn->systemml==1.3.0) (2.1.0)\n"
                }
            ],
            "source": "!pip install https://github.com/IBM/coursera/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/01/19 07:39:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n22/01/19 07:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n22/01/19 07:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n22/01/19 07:39:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
                }
            ],
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\nSo the first thing we need to ensure is that we are on the latest version of SystemML, which is 1.3.0 (as of 20th March'19) Please use the code block below to check if you are already on 1.3.0 or higher. 1.3 contains a necessary fix, that's we are running against the SNAPSHOT\n"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "mkdir: cannot create directory \u2018/home/dsxuser\u2019: Permission denied\r\n"
                }
            ],
            "source": "!mkdir -p /home/dsxuser/work/systemml\n"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nWelcome to Apache SystemML!\nVersion 1.3.0-SNAPSHOT\n1.3.0-SNAPSHOT\n"
                }
            ],
            "source": "from systemml import MLContext, dml\nml = MLContext(spark)\nml.setConfigProperty(\"sysml.localtmpdir\", \"mkdir /home/dsxuser/work/systemml\")\nprint(ml.version())\n    \nif not ml.version() == '1.3.0-SNAPSHOT':\n    raise ValueError('please upgrade to SystemML 1.3.0, or restart your Kernel (Kernel->Restart & Clear Output)')"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2022-01-19 07:39:43--  https://github.com/IBM/coursera/blob/master/coursera_ml/shake.parquet?raw=true\nResolving github.com (github.com)... 140.82.121.4\nConnecting to github.com (github.com)|140.82.121.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/claimed/blob/master/coursera_ml/shake.parquet?raw=true [following]\n--2022-01-19 07:39:44--  https://github.com/IBM/claimed/blob/master/coursera_ml/shake.parquet?raw=true\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github.com/IBM/claimed/raw/master/coursera_ml/shake.parquet [following]\n--2022-01-19 07:39:44--  https://github.com/IBM/claimed/raw/master/coursera_ml/shake.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/claimed/master/coursera_ml/shake.parquet [following]\n--2022-01-19 07:39:44--  https://raw.githubusercontent.com/IBM/claimed/master/coursera_ml/shake.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 74727 (73K) [application/octet-stream]\nSaving to: \u2018shake.parquet?raw=true\u2019\n\nshake.parquet?raw=t 100%[===================>]  72.98K  --.-KB/s    in 0.001s  \n\n2022-01-19 07:39:44 (49.8 MB/s) - \u2018shake.parquet?raw=true\u2019 saved [74727/74727]\n\n"
                }
            ],
            "source": "!wget https://github.com/IBM/coursera/blob/master/coursera_ml/shake.parquet?raw=true\n!mv shake.parquet?raw=true shake.parquet"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it\u2019s time to read the sensor data and create a temporary query table."
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "                                                                                \r"
                }
            ],
            "source": "df=spark.read.parquet('shake.parquet')"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+-----+---------+-----+-----+-----+\n|CLASS| SENSORID|    X|    Y|    Z|\n+-----+---------+-----+-----+-----+\n|    2| qqqqqqqq| 0.12| 0.12| 0.12|\n|    2|aUniqueID| 0.03| 0.03| 0.03|\n|    2| qqqqqqqq|-3.84|-3.84|-3.84|\n|    2| 12345678| -0.1| -0.1| -0.1|\n|    2| 12345678|-0.15|-0.15|-0.15|\n|    2| 12345678| 0.47| 0.47| 0.47|\n|    2| 12345678|-0.06|-0.06|-0.06|\n|    2| 12345678|-0.09|-0.09|-0.09|\n|    2| 12345678| 0.21| 0.21| 0.21|\n|    2| 12345678|-0.08|-0.08|-0.08|\n|    2| 12345678| 0.44| 0.44| 0.44|\n|    2|    gholi| 0.76| 0.76| 0.76|\n|    2|    gholi| 1.62| 1.62| 1.62|\n|    2|    gholi| 5.81| 5.81| 5.81|\n|    2| bcbcbcbc| 0.58| 0.58| 0.58|\n|    2| bcbcbcbc|-8.24|-8.24|-8.24|\n|    2| bcbcbcbc|-0.45|-0.45|-0.45|\n|    2| bcbcbcbc| 1.03| 1.03| 1.03|\n|    2|aUniqueID|-0.05|-0.05|-0.05|\n|    2| qqqqqqqq|-0.44|-0.44|-0.44|\n+-----+---------+-----+-----+-----+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pixiedust\n  Downloading pixiedust-1.1.19.tar.gz (197 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 197 kB 24.8 MB/s eta 0:00:01\n\u001b[?25hCollecting geojson\n  Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\nRequirement already satisfied: astunparse in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pixiedust) (1.6.3)\nRequirement already satisfied: markdown in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pixiedust) (3.1.1)\nCollecting colour\n  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pixiedust) (2.25.1)\nRequirement already satisfied: matplotlib in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pixiedust) (3.3.4)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pixiedust) (1.2.4)\nRequirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from astunparse->pixiedust) (1.15.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from astunparse->pixiedust) (0.35.1)\nRequirement already satisfied: setuptools>=36 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from markdown->pixiedust) (52.0.0.post20211006)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from matplotlib->pixiedust) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from matplotlib->pixiedust) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from matplotlib->pixiedust) (2.8.1)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from matplotlib->pixiedust) (8.4.0)\nRequirement already satisfied: numpy>=1.15 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from matplotlib->pixiedust) (1.19.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from matplotlib->pixiedust) (1.3.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from pandas->pixiedust) (2021.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->pixiedust) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->pixiedust) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->pixiedust) (1.26.6)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->pixiedust) (3.0.4)\nBuilding wheels for collected packages: pixiedust\n  Building wheel for pixiedust (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pixiedust: filename=pixiedust-1.1.19-py3-none-any.whl size=321803 sha256=158a44722e28e695bced8072b8fd1eea16fadf2cabfa4d1e4b71445c217ef778\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/a1/8f/a3/25f8e4f230bbbcc575afff7e6f13a90dd01c84cf03781af8f8\nSuccessfully built pixiedust\nInstalling collected packages: geojson, colour, pixiedust\nSuccessfully installed colour-0.1.5 geojson-2.5.0 pixiedust-1.1.19\n"
                }
            ],
            "source": "!pip install pixiedust"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "tableView"
                    }
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Pixiedust database opened successfully\nTable VERSION_TRACKER created successfully\nTable METRICS_TRACKER created successfully\n\nShare anonymous install statistics? (opt-out instructions)\n\nPixieDust will record metadata on its environment the next time the package is installed or updated. The data is anonymized and aggregated to help plan for future releases, and records only the following values:\n\n{\n   \"data_sent\": currentDate,\n   \"runtime\": \"python\",\n   \"application_version\": currentPixiedustVersion,\n   \"space_id\": nonIdentifyingUniqueId,\n   \"config\": {\n       \"repository_id\": \"https://github.com/ibm-watson-data-lab/pixiedust\",\n       \"target_runtimes\": [\"Data Science Experience\"],\n       \"event_id\": \"web\",\n       \"event_organizer\": \"dev-journeys\"\n   }\n}\nYou can opt out by calling pixiedust.optOut() in a new cell.\n"
                },
                {
                    "data": {
                        "text/html": "\n        <div style=\"margin:10px\">\n            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n            </a>\n            <span>Pixiedust version 1.1.19</span>\n        </div>\n        ",
                        "text/plain": "<IPython.core.display.HTML object>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\u001b[31mPixiedust runtime updated. Please restart kernel\u001b[0m\nTable SPARK_PACKAGES created successfully\nTable USER_PREFERENCES created successfully\nTable service_connections created successfully\n"
                },
                {
                    "data": {
                        "text/plain": "DataFrame[CLASS: bigint, SENSORID: string, X: double, Y: double, Z: double]"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": "import pixiedust\ndisplay(df)"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "df.createOrReplaceTempView(\"df\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We\u2019ll use Apache SystemML to implement Discrete Fourier Transformation. This way all computation continues to happen on the Apache Spark cluster for advanced scalability and performance."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As you\u2019ve learned from the lecture, implementing Discrete Fourier Transformation in a linear algebra programming language is simple. Apache SystemML DML is such a language and as you can see the implementation is straightforward and doesn\u2019t differ too much from the mathematical definition (Just note that the sum operator has been swapped with a vector dot product using the %*% syntax borrowed from R\n):\n\n<img style=\"float: left;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1af0a78dc50bbf118ab6bd4c4dcc3c4ff8502223\">\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "dml_script = '''\nPI = 3.141592654\nN = nrow(signal)\n\nn = seq(0, N-1, 1)\nk = seq(0, N-1, 1)\n\nM = (n %*% t(k))*(2*PI/N)\n\nXa = cos(M) %*% signal\nXb = sin(M) %*% signal\n\nDFT = cbind(Xa, Xb)\n'''"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it\u2019s time to create a function which takes a single row Apache Spark data frame as argument (the one containing the accelerometer measurement time series for one axis) and returns the Fourier transformation of it. In addition, we are adding an index column for later joining all axis together and renaming the columns to appropriate names. The result of this function is an Apache Spark DataFrame containing the Fourier Transformation of its input in two columns. \n"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.sql.functions import monotonically_increasing_id\n\ndef dft_systemml(signal,name):\n    prog = dml(dml_script).input('signal', signal).output('DFT')\n    \n    return (\n\n    #execute the script inside the SystemML engine running on top of Apache Spark\n    ml.execute(prog) \n     \n         #read result from SystemML execution back as SystemML Matrix\n        .get('DFT') \n     \n         #convert SystemML Matrix to ApacheSpark DataFrame \n        .toDF() \n     \n         #rename default column names\n        .selectExpr('C1 as %sa' % (name), 'C2 as %sb' % (name)) \n     \n         #add unique ID per row for later joining\n        .withColumn(\"id\", monotonically_increasing_id())\n    )\n        \n\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it\u2019s time to create individual DataFrames containing only a subset of the data. We filter simultaneously for accelerometer each sensor axis and one for each class. This means you\u2019ll get 6 DataFrames. Please implement this using the relational API of DataFrames or SparkSQL. Please use class 1 and 2 and not 0 and 1. <h1><span style=\"color:red\">Please make sure that each DataFrame has only ONE colum (only the measurement, eg. not CLASS column)</span></h1>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "x0 = spark.sql(\"SELECT X from df where class = 0\")\ny0 = spark.sql(\"SELECT Y from df where class = 0\")\nz0 = spark.sql(\"SELECT Z from df where class = 0\")\nx1 = spark.sql(\"SELECT X from df where class = 1\")\ny1 = spark.sql(\"SELECT Y from df where class = 1\")\nz1 = spark.sql(\"SELECT Z from df where class = 1\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Since we\u2019ve created this cool DFT function before, we can just call it for each of the 6 DataFrames now. And since the result of this function call is a DataFrame again we can use the pyspark best practice in simply calling methods on it sequentially. So what we are doing is the following:\n\n- Calling DFT for each class and accelerometer sensor axis.\n- Joining them together on the ID column. \n- Re-adding a column containing the class index.\n- Stacking both Dataframes for each classes together\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "SystemML Statistics:\nTotal execution time:\t\t0.011 sec.\nNumber of executed Spark inst:\t0.\n\nANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.7 used for parser compilation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.7 used for parser compilation does not match the current runtime version 4.8\n\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "SystemML Statistics:\nTotal execution time:\t\t0.000 sec.\nNumber of executed Spark inst:\t0.\n\n\nSystemML Statistics:\nTotal execution time:\t\t0.000 sec.\nNumber of executed Spark inst:\t0.\n\n\nSystemML Statistics:\nTotal execution time:\t\t0.404 sec.\nNumber of executed Spark inst:\t0.\n\n\nSystemML Statistics:\nTotal execution time:\t\t0.075 sec.\nNumber of executed Spark inst:\t0.\n\n\nSystemML Statistics:\nTotal execution time:\t\t0.112 sec.\nNumber of executed Spark inst:\t0.\n\n\n+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n|        id|                  xa|                  xb|                  ya|                  yb|                  za|                  zb|class|\n+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n|         0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|         6|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|         5|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|         1|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|         3|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|         2|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|         4|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|8589934592|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|8589934596|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|8589934598|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|8589934593|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|8589934594|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|8589934595|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|8589934597|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|\n|        26|0.007432298669951747|-0.00394663166701...|0.007432298669951747|-0.00394663166701...|0.007432298669951747|-0.00394663166701...|    1|\n|        29| 0.02589077158423112|-0.02414578651495463| 0.02589077158423112|-0.02414578651495463| 0.02589077158423112|-0.02414578651495463|    1|\n|        19|-0.08486126908675795| -0.0218913608396156|-0.08486126908675795| -0.0218913608396156|-0.08486126908675795| -0.0218913608396156|    1|\n|        54| 0.02941472472264519| 0.08753524593777502| 0.02941472472264519| 0.08753524593777502| 0.02941472472264519| 0.08753524593777502|    1|\n|         0|-0.00305531695825...|0.013615410059352576|-0.00305531695825...|0.013615410059352576|-0.00305531695825...|0.013615410059352576|    1|\n|        22| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232|    1|\n+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.sql.functions import lit\n\ndf_class_0 = dft_systemml(x0,'x') \\\n    .join(dft_systemml(y0,'y'), on=['id'], how='inner') \\\n    .join(dft_systemml(z0,'z'), on=['id'], how='inner') \\\n    .withColumn('class', lit(0))\n    \ndf_class_1 = dft_systemml(x1,'x') \\\n    .join(dft_systemml(y1,'y'), on=['id'], how='inner') \\\n    .join(dft_systemml(z1,'z'), on=['id'], how='inner') \\\n    .withColumn('class', lit(1))\n\ndf_dft = df_class_0.union(df_class_1)\n\ndf_dft.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Please create a VectorAssembler which consumes the newly created DFT columns and produces a column \u201cfeatures\u201d\n"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import VectorAssembler"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "vectorAssembler = VectorAssembler(\n    inputCols=[\"xa\", \"xb\", \"ya\", \"yb\", \"za\", \"zb\"],\n    outputCol=\"features\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Please insatiate a classifier from the SparkML package and assign it to the classifier variable. Make sure to set the \u201cclass\u201d column as target.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.classification import GBTClassifier"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.classification import RandomForestClassifier\n\nclassifier = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\", numTrees=10)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let\u2019s train and evaluate\u2026\n"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, classifier])"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "model = pipeline.fit(df_dft)"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "prediction = model.transform(df_dft)"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+-------------+-----------+----------+\n|        id|                  xa|                  xb|                  ya|                  yb|                  za|                  zb|class|            features|rawPrediction|probability|prediction|\n+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+-------------+-----------+----------+\n|         0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|         6|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|         5|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|         1|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|         3|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|         2|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|         4|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|8589934592|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|8589934596|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|8589934598|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|8589934593|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|8589934594|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|8589934595|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|8589934597|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|                 0.0|    0|           (6,[],[])|   [10.0,0.0]|  [1.0,0.0]|       0.0|\n|        26|0.007432298669951747|-0.00394663166701...|0.007432298669951747|-0.00394663166701...|0.007432298669951747|-0.00394663166701...|    1|[0.00743229866995...|   [0.0,10.0]|  [0.0,1.0]|       1.0|\n|        29| 0.02589077158423112|-0.02414578651495463| 0.02589077158423112|-0.02414578651495463| 0.02589077158423112|-0.02414578651495463|    1|[0.02589077158423...|   [0.0,10.0]|  [0.0,1.0]|       1.0|\n|        19|-0.08486126908675795| -0.0218913608396156|-0.08486126908675795| -0.0218913608396156|-0.08486126908675795| -0.0218913608396156|    1|[-0.0848612690867...|   [0.0,10.0]|  [0.0,1.0]|       1.0|\n|        54| 0.02941472472264519| 0.08753524593777502| 0.02941472472264519| 0.08753524593777502| 0.02941472472264519| 0.08753524593777502|    1|[0.02941472472264...|   [0.0,10.0]|  [0.0,1.0]|       1.0|\n|         0|-0.00305531695825...|0.013615410059352576|-0.00305531695825...|0.013615410059352576|-0.00305531695825...|0.013615410059352576|    1|[-0.0030553169582...|   [0.0,10.0]|  [0.0,1.0]|       1.0|\n|        22| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232|    1|[0.03912775214058...|   [0.0,10.0]|  [0.0,1.0]|       1.0|\n+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+-------------+-----------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "prediction.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "1.0"
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"class\")\n    \nbinEval.evaluate(prediction) "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "If you are happy with the result (I\u2019m happy with > 0.8) please submit your solution to the grader by executing the following cells, please don\u2019t forget to obtain an assignment submission token (secret) from the Courera\u2019s graders web page and paste it to the \u201csecret\u201d variable below, including your email address you\u2019ve used for Coursera. \n"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "!rm -Rf a2_m4.json"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "prediction = prediction.repartition(1)\nprediction.write.json('a2_m4.json')"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2022-01-19 07:41:44--  http://wget/\nResolving wget (wget)... failed: Name or service not known.\nwget: unable to resolve host address \u2018wget\u2019\n--2022-01-19 07:41:44--  https://raw.githubusercontent.com/IBM/coursera/master/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2540 (2.5K) [text/plain]\nSaving to: \u2018rklib.py\u2019\n\nrklib.py            100%[===================>]   2.48K  --.-KB/s    in 0s      \n\n2022-01-19 07:41:44 (42.9 MB/s) - \u2018rklib.py\u2019 saved [2540/2540]\n\nFINISHED --2022-01-19 07:41:44--\nTotal wall clock time: 0.2s\nDownloaded: 1 files, 2.5K in 0s (42.9 MB/s)\n"
                }
            ],
            "source": "!rm -f rklib.py\n!wget wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "from rklib import zipit\nzipit('a2_m4.json.zip','a2_m4.json')"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "!base64 a2_m4.json.zip > a2_m4.json.zip.base64"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Submission successful, please check on the coursera grader page for the status\n-------------------------\n{\"elements\":[{\"itemId\":\"B8wXV\",\"id\":\"f_F-qCtuEei_fRLwaVDk3g~B8wXV~VOhYDnj7EeywtgrLqOXVzQ\",\"courseId\":\"f_F-qCtuEei_fRLwaVDk3g\"}],\"paging\":{},\"linked\":{}}\n-------------------------\n"
                }
            ],
            "source": "from rklib import submit\nkey = \"-fBiYHYDEeiR4QqiFhAvkA\"\npart = \"IjtJk\"\nemail = \"rapidhunter250@gmail.com\"\nsubmission_token = \"Q1EywKvmIyTTHx73\"\nwith open('a2_m4.json.zip.base64', 'r') as myfile:\n    data=myfile.read()\nsubmit(email, submission_token, key, part, [part], data)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}